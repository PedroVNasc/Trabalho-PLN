{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solução simbólica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação do dicionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import TypedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(TypedDict):\n",
    "    raiz: set[str]\n",
    "    grammar_class: set[str]\n",
    "    morphemes: set[str]\n",
    "\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, head: str, data: dict | None = None, prox: dict[str, 'Node'] | None = None):\n",
    "        self.head = head\n",
    "        self.data = data\n",
    "        self.prox = {} if prox is None else prox\n",
    "\n",
    "\n",
    "class ReGra():\n",
    "    def __init__(self):\n",
    "        self.nodes: dict[str, Node] = {}\n",
    "\n",
    "    def dive(self, current_node: Node, word: str, depth: int):\n",
    "        # Palavra não existe ou palavra encontrada\n",
    "        if depth >= len(word) or word[depth] not in current_node.prox.keys():\n",
    "            return current_node\n",
    "\n",
    "        # Continua a recursão\n",
    "        return self.dive(current_node.prox[word[depth]], word, depth + 1)\n",
    "\n",
    "    def get_parent(self, word: str) -> Node | None:\n",
    "        if word[0] not in self.nodes.keys():\n",
    "            return None\n",
    "\n",
    "        return self.dive(self.nodes[word[0]], word, 1)\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        if word[0] not in self.nodes.keys():\n",
    "            return None\n",
    "\n",
    "        node = self.dive(self.nodes[word[0]], word, 1)\n",
    "\n",
    "        if node.head != word:\n",
    "            return None\n",
    "\n",
    "        return node.data\n",
    "\n",
    "    def __setitem__(self, idx: str, value: WordData):\n",
    "        # #print()\n",
    "        # print()\n",
    "        parent = self.get_parent(idx)\n",
    "\n",
    "        # Se o conjunto está vazio ou uma letra base não existe\n",
    "        if parent is None:\n",
    "            if len(idx) == 1:\n",
    "                self.nodes[idx] = Node(head=idx, data=value)\n",
    "                return\n",
    "\n",
    "            parent = Node(head=idx[0])\n",
    "            self.nodes[idx[0]] = parent\n",
    "\n",
    "            for i in range(1, len(idx) - 1):\n",
    "                parent.prox[idx[i]] = Node(head=idx[:i + 1])\n",
    "                parent = parent.prox[idx[i]]\n",
    "\n",
    "            parent.prox[idx[-1]] = Node(head=idx, data=value)\n",
    "            return\n",
    "\n",
    "        # Se a palavra existe, atualiza os dados\n",
    "        if parent.head == idx:\n",
    "            if parent.data is None:\n",
    "                parent.data = value\n",
    "\n",
    "            else:\n",
    "                parent.data['raiz'] = parent.data['raiz'].union(value['raiz'])\n",
    "                parent.data['grammar_class'] = parent.data['grammar_class'].union(\n",
    "                    value['grammar_class'])\n",
    "                parent.data['morphemes'] = parent.data['morphemes'].union(\n",
    "                    value['morphemes'])\n",
    "\n",
    "            return\n",
    "\n",
    "        # Se a palavra não existe, mas é filha de uma que existe\n",
    "        # cria o nó dela\n",
    "        if len(parent.head) + 1 == len(idx):\n",
    "            parent.prox[idx[-1]] = Node(head=idx, data=value)\n",
    "\n",
    "        # Se a palavra não existe, e não é filha de uma que existe\n",
    "        # cria um caminho de nós até ela\n",
    "        else:\n",
    "            for i in range(len(parent.head), len(idx) - 1):\n",
    "                parent.prox[idx[i]] = Node(head=idx[:i + 1])\n",
    "                parent = parent.prox[idx[i]]\n",
    "\n",
    "            parent.prox[idx[-1]] = Node(head=idx, data=value)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        string = ''\n",
    "        for node in self.nodes.values():\n",
    "            string += self.build_tree(node)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def build_tree(self, current: Node, depth: int = 0):\n",
    "        string = f\"{'-' * depth} {current.head}{' OK' if current.data is not None else ''}\\n\"\n",
    "\n",
    "        for node in current.prox.values():\n",
    "            string += self.build_tree(node, depth + 1)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def get_children(self, current: Node, max_depth: int = 10, depth: int = 0) -> list[str]:\n",
    "        if depth >= max_depth:\n",
    "            return []\n",
    "\n",
    "        children = []\n",
    "\n",
    "        for node in current.prox.values():\n",
    "            if node.data is not None:\n",
    "                children.append(node.head)\n",
    "\n",
    "            children += self.get_children(node, max_depth=max_depth, depth=depth + 1)\n",
    "\n",
    "        return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionary_path = \"./portilexicon-ud.tsv\"\n",
    "dictionary = ReGra()\n",
    "\n",
    "with open(dicionary_path) as dic:\n",
    "    rd = csv.reader(dic, delimiter='\\t', quotechar='\"')\n",
    "    qtd_lines = sum(1 for _ in rd)\n",
    "    dic.seek(0)    \n",
    "    print('teste')\n",
    "    \n",
    "    progress = tqdm(total=qtd_lines, desc='Carregando dicionário')\n",
    "    \n",
    "    for row in rd:\n",
    "        word = row[0]\n",
    "        raiz = [row[1]]\n",
    "        grammar_class = [row[2]]\n",
    "        morphemes = [x for x in row[3].split('|') if x]\n",
    "\n",
    "        dictionary[word] = WordData(raiz=set(raiz), grammar_class=set(grammar_class), morphemes=set(morphemes))\n",
    "        \n",
    "        progress.update(1)\n",
    "        \n",
    "    progress.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing da Frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    return text.lower().replace(',', '').replace(\n",
    "        '.', '').replace('!', '').replace('?', '').replace(';', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(phrase: str, dictionary: ReGra):\n",
    "    phrase = clean_text(phrase)\n",
    "    \n",
    "    out = {}\n",
    "    for word in phrase.split():\n",
    "        out[word] = dictionary[word]\n",
    "        \n",
    "        if out[word] is None:\n",
    "            out[word] = {'raiz': set(['Unknown']), 'grammar_class': set(['Unknown']), 'morphemes': set(['Unknown'])}\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração da Árvore Sintática\n",
    "\n",
    "A árvore sintática é gerada através do LX-Parser. Para obter um chave da API siga as instruções em:\n",
    "\n",
    "[https://portulanclarin.net/workbench/lx-parser/](https://portulanclarin.net/workbench/lx-parser/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LXPARSER_WS_API_URL = 'https://portulanclarin.net/workbench/lx-parser/api/'\n",
    "\n",
    "LXPARSER_WS_API_KEY = ''\n",
    "with open('.lxparser-key', 'r') as f: LXPARSER_WS_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função fornecida pelos desenvolvedores do LX-Parser\n",
    "\n",
    "class WSException(Exception):\n",
    "    'Webservice Exception'\n",
    "    def __init__(self, errordata):\n",
    "        \"errordata is a dict returned by the webservice with details about the error\"\n",
    "        super().__init__(self)\n",
    "        assert isinstance(errordata, dict)\n",
    "        self.message = errordata[\"message\"]\n",
    "        # see https://json-rpc.readthedocs.io/en/latest/exceptions.html for more info\n",
    "        # about JSON-RPC error codes\n",
    "        if -32099 <= errordata[\"code\"] <= -32000:  # Server Error\n",
    "            if errordata[\"data\"][\"type\"] == \"WebServiceException\":\n",
    "                self.message += f\": {errordata['data']['message']}\"\n",
    "            else:\n",
    "                self.message += f\": {errordata['data']!r}\"\n",
    "    def __str__(self):\n",
    "        return self.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função fornecida pelos desenvolvedores do LX-Parser\n",
    "\n",
    "def parse(text, format):\n",
    "    '''\n",
    "    Arguments\n",
    "        text: a string with a maximum of 2000 characters, Portuguese text, with\n",
    "             the input to be processed\n",
    "        format: either 'parentheses', 'table' or 'JSON'\n",
    "\n",
    "    Returns a string or JSON object with the output according to specification in\n",
    "       https://portulanclarin.net/workbench/lx-parser/\n",
    "    \n",
    "    Raises a WSException if an error occurs.\n",
    "    '''\n",
    "\n",
    "    request_data = {\n",
    "        'method': 'parse',\n",
    "        'jsonrpc': '2.0',\n",
    "        'id': 0,\n",
    "        'params': {\n",
    "            'text': text,\n",
    "            'format': format,\n",
    "            'key': LXPARSER_WS_API_KEY,\n",
    "        },\n",
    "    }\n",
    "    request = requests.post(LXPARSER_WS_API_URL, json=request_data)\n",
    "    response_data = request.json()\n",
    "    if \"error\" in response_data:\n",
    "        raise WSException(response_data[\"error\"])\n",
    "    else:\n",
    "        return response_data[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_teste = \"O que você quer dar a ela, Pedro? dasdasd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = parse(frase_teste, format=\"table\")\n",
    "print(result_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alinhando o dicionário e o LX-Parser\n",
    "\n",
    "Como eles utilizam símbolos diferentes, é necessário alinhar as classes antes de prosseguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_corvertion = {\n",
    "    'A': ['ADJ'],\n",
    "    'ART': ['DET', 'PRON'],\n",
    "    'N': ['NOUN'],\n",
    "    'QNT': ['PRON'],\n",
    "    'ADV': ['ADV'],\n",
    "    'P': ['PRON'],\n",
    "    'REL': ['CCONJ', 'SCONJ'],\n",
    "    'V': ['VERB'],\n",
    "    'PRS': ['PRON'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result_table.split('\\n')\n",
    "\n",
    "predicted = []\n",
    "for r in result:\n",
    "    r = r.split('\\t')\n",
    "    r[1] = r[1].replace(')', '').split('(')[-1].replace('*', '')\n",
    "    \n",
    "    if len(r) > 1 and r[1] != 'PNT':\n",
    "        predicted.append((r[0], symbol_corvertion[r[1]]))\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = [[x[0], x[1]['grammar_class']] for x in parsing(frase_teste, dictionary).items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligning(parsed, predicted):\n",
    "    if len(parsed) != len(predicted):\n",
    "        print(f\"Lengths don't match: {len(parsed)} != {len(predicted)}\")\n",
    "        return []\n",
    "\n",
    "    aligned_classes = []\n",
    "\n",
    "    for i in range(len(parsed)):\n",
    "        if parsed[i][0] != predicted[i][0].lower():\n",
    "            print(f\"Words don't match: {parsed[i][0]} != {predicted[i][0]}\")\n",
    "            return []\n",
    "        \n",
    "        word = predicted[i][0]\n",
    "        classes = parsed[i][1].intersection(set(predicted[i][1]))\n",
    "        \n",
    "        \n",
    "        if len(classes) == 0:\n",
    "            # Para o caso de ser um nome\n",
    "            if 'Unknown' in parsed[i][1] and 'NOUN' in predicted[i][1]:\n",
    "                classes = set(predicted[i][1])\n",
    "            \n",
    "            else:\n",
    "                classes = set(['ERROR', predicted[i][1][0]])\n",
    "\n",
    "        aligned_classes.append([word, classes])\n",
    "\n",
    "    return aligned_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_tree = aligning(parsed, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in aligned_tree: print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sugere as mudanças"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correção gramatical\n",
    "\n",
    "Calcula a diferença entre a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(word1, word2):\n",
    "    len1 = len(word1)\n",
    "    len2 = len(word2)\n",
    "\n",
    "    matrix_len = max(len1, len2) + 1\n",
    "    matrix = [[0 for _ in range(matrix_len)] for _ in range(matrix_len)]\n",
    "\n",
    "    for i in range(matrix_len):\n",
    "        matrix[i][0] = i\n",
    "        matrix[0][i] = i\n",
    "\n",
    "    for i in range(1, matrix_len):\n",
    "        for j in range(1, matrix_len):\n",
    "            matrix[i][j] = min(matrix[i - 1][j], matrix[i][j - 1], matrix[i - 1][j - 1])\n",
    "            \n",
    "            if i > len1:\n",
    "                matrix[i][j] += 1\n",
    "            elif j > len2:\n",
    "                matrix[i][j] += 1\n",
    "            elif word1[i - 1] != word2[j - 1]:\n",
    "                matrix[i][j] += 1\n",
    "\n",
    "    return matrix[matrix_len - 1][matrix_len - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, grammar_class, dictionary):\n",
    "    word = clean_text(word)\n",
    "    p = dictionary.get_parent(word)\n",
    "    \n",
    "    print(grammar_class)\n",
    "\n",
    "    # Procura paralavras com 2 até dois caracteres a mais ou a menos\n",
    "    if len(p.head) > 2:\n",
    "        p = dictionary.get_parent(p.head[:-2])\n",
    "\n",
    "    children = dictionary.get_children(p, max_depth=4.0)\n",
    "\n",
    "    corrections = []\n",
    "    for c in children:\n",
    "        if grammar_class in dictionary[c]['grammar_class']:\n",
    "            corrections.append(c)\n",
    "\n",
    "    corrections.sort(key=lambda x: levenshtein_distance(word, x))\n",
    "    return corrections[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_corrections = {}\n",
    "\n",
    "for w in aligned_tree:\n",
    "    if 'ERROR' in w[1]:\n",
    "        possible_corrections[w[0]] = get_corrections(w[0], list(w[1] - {'ERROR'})[0], dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define uma classe para correção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corretor import Corretor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionary_path = \"./portilexicon-ud.tsv\"\n",
    "\n",
    "LXPARSER_WS_API_KEY = ''\n",
    "with open('.lxparser-key', 'r') as f: LXPARSER_WS_API_KEY = f.read()\n",
    "\n",
    "symbol_corvertion = {\n",
    "    'A': ['ADJ'],\n",
    "    'ART': ['DET', 'PRON'],\n",
    "    'N': ['NOUN'],\n",
    "    'QNT': ['PRON'],\n",
    "    'ADV': ['ADV'],\n",
    "    'P': ['PRON'],\n",
    "    'REL': ['CCONJ', 'SCONJ'],\n",
    "    'V': ['VERB'],\n",
    "    'PRS': ['PRON'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_corretor = Corretor(dicionary_path)\n",
    "teste_corretor.setup_key(LXPARSER_WS_API_KEY)\n",
    "teste_corretor.setup_symbols(symbol_corvertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_corretor.corrigir_texto(\"Um cachorro quente bem fresquinhu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilização de LLM + Marcação para reescrever mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path='./sabia-7b.Q4_0.gguf',\n",
    "    seed=42,\n",
    "    chat_format=\"llama-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(question: str, context: str | None, full_output=False, seed=42):\n",
    "    context = [\n",
    "        'Você um excelente assistente que reescreve textos segundo bons padrões. Abaixo estão os deve reescrever com mais seriedade:',\n",
    "        'Q: Onde que a gente pode comprar ingressos pro show?',\n",
    "        'A: Onde podemos comprar ingressos para o show?',\n",
    "        'Q: Por que que o céu é azul?',\n",
    "        'A: Por que o céu é azul?',\n",
    "    ] if context is None else context\n",
    "\n",
    "    context.append('Q: ' + question)\n",
    "    context.append('A:')\n",
    "\n",
    "    input_text = '\\n'.join(context)\n",
    "\n",
    "    # É estimado que a razão de tokens por palavra seja 0.75, \n",
    "    # então usa um número maior por segurança\n",
    "    # max_tokens = len(question.split()) * 3 + 1\n",
    "\n",
    "    output = llm(\n",
    "        input_text,\n",
    "        max_tokens=32,\n",
    "        stop=[\"Q:\"],\n",
    "        # seed=seed,\n",
    "        echo=True,\n",
    "    )\n",
    "\n",
    "    return output['choices'][0]['text'].split(':')[-1] if not full_output else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rewrite(\"Cachorro quente fresquinhão\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corretor import Corretor\n",
    "\n",
    "dicionary_path = \"./portilexicon-ud.tsv\"\n",
    "\n",
    "LXPARSER_WS_API_KEY = ''\n",
    "with open('.lxparser-key', 'r') as f: LXPARSER_WS_API_KEY = f.read()\n",
    "\n",
    "symbol_corvertion = {\n",
    "    'A': ['ADJ'],\n",
    "    'ART': ['DET', 'PRON'],\n",
    "    'N': ['NOUN'],\n",
    "    'QNT': ['PRON'],\n",
    "    'ADV': ['ADV'],\n",
    "    'P': ['PRON'],\n",
    "    'REL': ['CCONJ', 'SCONJ'],\n",
    "    'V': ['VERB'],\n",
    "    'PRS': ['PRON'],\n",
    "}\n",
    "\n",
    "corretor = Corretor(dicionary_path, LXPARSER_WS_API_KEY, symbol_corvertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_message = 'Cachorro quente fresquinhu aqui'\n",
    "\n",
    "out = corretor.corrigir_texto(test_message)\n",
    "\n",
    "rew = rewrite(test_message, None)\n",
    "\n",
    "out2 = corretor.corrigir_texto(rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)\n",
    "print()\n",
    "print(rew)\n",
    "print()\n",
    "print(out2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
