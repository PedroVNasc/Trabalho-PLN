{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solução simbólica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação do dicionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import TypedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(TypedDict):\n",
    "    raiz: set[str]\n",
    "    tag: set[str]\n",
    "    features: set[str]\n",
    "\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, head: str, data: dict | None = None, prox: dict[str, 'Node'] | None = None):\n",
    "        self.head = head\n",
    "        self.data = data\n",
    "        self.prox = {} if prox is None else prox\n",
    "\n",
    "\n",
    "class ReGra():\n",
    "    def __init__(self):\n",
    "        self.nodes: dict[str, Node] = {}\n",
    "\n",
    "    def dive(self, current_node: Node, word: str, depth: int):\n",
    "        # Palavra não existe ou palavra encontrada\n",
    "        if depth >= len(word) or word[depth] not in current_node.prox.keys():\n",
    "            return current_node\n",
    "\n",
    "        # Continua a recursão\n",
    "        return self.dive(current_node.prox[word[depth]], word, depth + 1)\n",
    "\n",
    "    def get_parent(self, word: str) -> Node | None:\n",
    "        if word[0] not in self.nodes.keys():\n",
    "            return None\n",
    "\n",
    "        return self.dive(self.nodes[word[0]], word, 1)\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        if word[0] not in self.nodes.keys():\n",
    "            return None\n",
    "\n",
    "        node = self.dive(self.nodes[word[0]], word, 1)\n",
    "\n",
    "        if node.head != word:\n",
    "            return None\n",
    "\n",
    "        return node.data\n",
    "\n",
    "    def __setitem__(self, idx: str, value: WordData):\n",
    "        # #print()\n",
    "        # print()\n",
    "        parent = self.get_parent(idx)\n",
    "\n",
    "        # Se o conjunto está vazio ou uma letra base não existe\n",
    "        if parent is None:\n",
    "            if len(idx) == 1:\n",
    "                self.nodes[idx] = Node(head=idx, data=value)\n",
    "                return\n",
    "\n",
    "            parent = Node(head=idx[0])\n",
    "            self.nodes[idx[0]] = parent\n",
    "\n",
    "            for i in range(1, len(idx) - 1):\n",
    "                parent.prox[idx[i]] = Node(head=idx[:i + 1])\n",
    "                parent = parent.prox[idx[i]]\n",
    "\n",
    "            parent.prox[idx[-1]] = Node(head=idx, data=value)\n",
    "            return\n",
    "\n",
    "        # Se a palavra existe, atualiza os dados\n",
    "        if parent.head == idx:\n",
    "            if parent.data is None:\n",
    "                parent.data = value\n",
    "\n",
    "            else:\n",
    "                parent.data['raiz'] = parent.data['raiz'].union(value['raiz'])\n",
    "                parent.data['tag'] = parent.data['tag'].union(\n",
    "                    value['tag'])\n",
    "                parent.data['features'] = parent.data['features'].union(\n",
    "                    value['features'])\n",
    "\n",
    "            return\n",
    "\n",
    "        # Se a palavra não existe, mas é filha de uma que existe\n",
    "        # cria o nó dela\n",
    "        if len(parent.head) + 1 == len(idx):\n",
    "            parent.prox[idx[-1]] = Node(head=idx, data=value)\n",
    "\n",
    "        # Se a palavra não existe, e não é filha de uma que existe\n",
    "        # cria um caminho de nós até ela\n",
    "        else:\n",
    "            for i in range(len(parent.head), len(idx) - 1):\n",
    "                parent.prox[idx[i]] = Node(head=idx[:i + 1])\n",
    "                parent = parent.prox[idx[i]]\n",
    "\n",
    "            parent.prox[idx[-1]] = Node(head=idx, data=value)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        string = ''\n",
    "        for node in self.nodes.values():\n",
    "            string += self.build_tree(node)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def build_tree(self, current: Node, depth: int = 0):\n",
    "        string = f\"{'-' * depth} {current.head}{' OK' if current.data is not None else ''}\\n\"\n",
    "\n",
    "        for node in current.prox.values():\n",
    "            string += self.build_tree(node, depth + 1)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def get_children(self, current: Node, max_depth: int = 10, depth: int = 0) -> list[str]:\n",
    "        if depth >= max_depth:\n",
    "            return []\n",
    "\n",
    "        children = []\n",
    "\n",
    "        for node in current.prox.values():\n",
    "            if node.data is not None:\n",
    "                children.append(node.head)\n",
    "\n",
    "            children += self.get_children(node, max_depth=max_depth, depth=depth + 1)\n",
    "\n",
    "        return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionary_path = \"./portilexicon-ud.tsv\"\n",
    "dictionary = ReGra()\n",
    "\n",
    "with open(dicionary_path) as dic:\n",
    "    rd = csv.reader(dic, delimiter='\\t', quotechar='\"')\n",
    "    qtd_lines = sum(1 for _ in rd)\n",
    "    dic.seek(0)    \n",
    "    print('teste')\n",
    "    \n",
    "    progress = tqdm(total=qtd_lines, desc='Carregando dicionário')\n",
    "    \n",
    "    for row in rd:\n",
    "        word = row[0]\n",
    "        raiz = [row[1]]\n",
    "        tag = [row[2]]\n",
    "        features = [x for x in row[3].split('|') if x]\n",
    "\n",
    "        dictionary[word] = WordData(raiz=set(raiz), tag=set(tag), features=set(features))\n",
    "        \n",
    "        progress.update(1)\n",
    "        \n",
    "    progress.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing da Frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    return text.lower().replace(',', '').replace(\n",
    "        '.', '').replace('!', '').replace('?', '').replace(';', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(phrase: str, dictionary: ReGra):\n",
    "    phrase = clean_text(phrase)\n",
    "    \n",
    "    out = {}\n",
    "    for word in phrase.split():\n",
    "        out[word] = dictionary[word]\n",
    "        \n",
    "        if out[word] is None:\n",
    "            out[word] = {'raiz': set(['Unknown']), 'tag': set(['Unknown']), 'features': set(['Unknown'])}\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração da Árvore Sintática\n",
    "\n",
    "A árvore sintática é gerada através do LX-Parser. Para obter um chave da API siga as instruções em:\n",
    "\n",
    "[https://portulanclarin.net/workbench/lx-parser/](https://portulanclarin.net/workbench/lx-parser/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LXPARSER_WS_API_URL = 'https://portulanclarin.net/workbench/lx-parser/api/'\n",
    "\n",
    "LXPARSER_WS_API_KEY = ''\n",
    "with open('.lxparser-key', 'r') as f: LXPARSER_WS_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função fornecida pelos desenvolvedores do LX-Parser\n",
    "\n",
    "class WSException(Exception):\n",
    "    'Webservice Exception'\n",
    "    def __init__(self, errordata):\n",
    "        \"errordata is a dict returned by the webservice with details about the error\"\n",
    "        super().__init__(self)\n",
    "        assert isinstance(errordata, dict)\n",
    "        self.message = errordata[\"message\"]\n",
    "        # see https://json-rpc.readthedocs.io/en/latest/exceptions.html for more info\n",
    "        # about JSON-RPC error codes\n",
    "        if -32099 <= errordata[\"code\"] <= -32000:  # Server Error\n",
    "            if errordata[\"data\"][\"type\"] == \"WebServiceException\":\n",
    "                self.message += f\": {errordata['data']['message']}\"\n",
    "            else:\n",
    "                self.message += f\": {errordata['data']!r}\"\n",
    "    def __str__(self):\n",
    "        return self.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função fornecida pelos desenvolvedores do LX-Parser\n",
    "\n",
    "def parse(text, format):\n",
    "    '''\n",
    "    Arguments\n",
    "        text: a string with a maximum of 2000 characters, Portuguese text, with\n",
    "             the input to be processed\n",
    "        format: either 'parentheses', 'table' or 'JSON'\n",
    "\n",
    "    Returns a string or JSON object with the output according to specification in\n",
    "       https://portulanclarin.net/workbench/lx-parser/\n",
    "    \n",
    "    Raises a WSException if an error occurs.\n",
    "    '''\n",
    "\n",
    "    request_data = {\n",
    "        'method': 'parse',\n",
    "        'jsonrpc': '2.0',\n",
    "        'id': 0,\n",
    "        'params': {\n",
    "            'text': text,\n",
    "            'format': format,\n",
    "            'key': LXPARSER_WS_API_KEY,\n",
    "        },\n",
    "    }\n",
    "    request = requests.post(LXPARSER_WS_API_URL, json=request_data)\n",
    "    response_data = request.json()\n",
    "    if \"error\" in response_data:\n",
    "        raise WSException(response_data[\"error\"])\n",
    "    else:\n",
    "        return response_data[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_teste = \"Cacharros quentes quentinhus aqui\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = parse(frase_teste, format=\"parentheses\")\n",
    "print(result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import svgling\n",
    "from nltk import Tree\n",
    "\n",
    "tree: Tree = []\n",
    "for sentence in result_table.splitlines(keepends=False):\n",
    "    tree = Tree.fromstring(sentence)\n",
    "    IPython.display.display(svgling.draw_tree(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alinhando o dicionário e o LX-Parser\n",
    "\n",
    "Como eles utilizam símbolos diferentes, é necessário alinhar as classes antes de prosseguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_convertion = {\n",
    "    'A': ['ADJ'],\n",
    "    'ART': ['DET', 'PRON'],\n",
    "    'N': ['NOUN'],\n",
    "    'QNT': ['PRON'],\n",
    "    'ADV': ['ADV'],\n",
    "    'P': ['PRON'],\n",
    "    'REL': ['CCONJ', 'SCONJ'],\n",
    "    'V': ['VERB'],\n",
    "    'PRS': ['PRON'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for word in tree.pos():\n",
    "    if word[1] != 'PNT':\n",
    "        predicted.append([word[0], symbol_convertion[word[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = [[x[0], x[1]['tag']] for x in parsing(frase_teste, dictionary).items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligning(parsed, predicted):\n",
    "    if len(parsed) != len(predicted):\n",
    "        print(f\"Lengths don't match: {len(parsed)} != {len(predicted)}\")\n",
    "        return []\n",
    "\n",
    "    aligned_classes = []\n",
    "\n",
    "    for i in range(len(parsed)):\n",
    "        if parsed[i][0] != predicted[i][0].lower():\n",
    "            print(f\"Words don't match: {parsed[i][0]} != {predicted[i][0]}\")\n",
    "            return []\n",
    "\n",
    "        word = predicted[i][0]\n",
    "        classes = parsed[i][1].intersection(set(predicted[i][1]))\n",
    "\n",
    "        if len(classes) == 0:\n",
    "            if 'Unknown' in parsed[i][1]:\n",
    "                # Cobre a possibilidade de ser um nome\n",
    "                if 'NOUN' in predicted[i][1]:\n",
    "                    classes = set(predicted[i][1])\n",
    "\n",
    "                else:  # Do contrário, é um erro\n",
    "                    classes = set(['ERROR', predicted[i][1][0]])\n",
    "\n",
    "            else:  # Mitiga problemas com a imprecisão do parser\n",
    "                classes = parsed[i][1]\n",
    "\n",
    "        aligned_classes.append([word, classes])\n",
    "\n",
    "    return aligned_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_tree = aligning(parsed, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in aligned_tree: print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sugere as mudanças"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correção gramatical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(word1, word2):\n",
    "    len1 = len(word1)\n",
    "    len2 = len(word2)\n",
    "\n",
    "    matrix_len = max(len1, len2) + 1\n",
    "    matrix = [[0 for _ in range(matrix_len)] for _ in range(matrix_len)]\n",
    "\n",
    "    for i in range(matrix_len):\n",
    "        matrix[i][0] = i\n",
    "        matrix[0][i] = i\n",
    "\n",
    "    for i in range(1, matrix_len):\n",
    "        for j in range(1, matrix_len):\n",
    "            matrix[i][j] = min(matrix[i - 1][j], matrix[i][j - 1], matrix[i - 1][j - 1])\n",
    "            \n",
    "            if i > len1:\n",
    "                matrix[i][j] += 1\n",
    "            elif j > len2:\n",
    "                matrix[i][j] += 1\n",
    "            elif word1[i - 1] != word2[j - 1]:\n",
    "                matrix[i][j] += 1\n",
    "\n",
    "    return matrix[matrix_len - 1][matrix_len - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, tag, dictionary):\n",
    "    word = clean_text(word)\n",
    "    p = dictionary.get_parent(word)\n",
    "    \n",
    "    print(tag)\n",
    "\n",
    "    # Procura paralavras com 2 até dois caracteres a mais ou a menos\n",
    "    if len(p.head) > 2:\n",
    "        p = dictionary.get_parent(p.head[:-2])\n",
    "\n",
    "    max_depth = max(len(word) - len(p.head) + 2, 4.0)\n",
    "    children = dictionary.get_children(p, max_depth=max_depth)\n",
    "\n",
    "    corrections = []\n",
    "    for c in children:\n",
    "        if tag == 'ANY' or tag in dictionary[c]['tag']:\n",
    "            corrections.append(c)\n",
    "\n",
    "    corrections.sort(key=lambda x: levenshtein_distance(word, x))\n",
    "    return corrections[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_corrections = {}\n",
    "\n",
    "for w in aligned_tree:\n",
    "    if 'ERROR' in w[1]:\n",
    "        tag = list(w[1] - {'ERROR'})[0]\n",
    "        possible_corrections[w[0]] = get_corrections(w[0], tag, dictionary)\n",
    "        \n",
    "        if len(possible_corrections[w[0]]) == 0:\n",
    "            possible_corrections[w[0]] = get_corrections(w[0], 'ANY', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(possible_corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorte das melhores soluções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_details(details):\n",
    "    details_dict = {}\n",
    "\n",
    "    for x in details:\n",
    "        if x == '_':\n",
    "            continue\n",
    "\n",
    "        char, value = x.split('=')\n",
    "        if char in details_dict:\n",
    "            details_dict[char].append(value)\n",
    "        else:\n",
    "            details_dict[char] = [value]\n",
    "\n",
    "    return details_dict\n",
    "\n",
    "def prune_corrections(corrections, tree: Tree):\n",
    "    details = set()\n",
    "    for x in tree.leaves():\n",
    "        d = dictionary[x]\n",
    "\n",
    "        if d is not None:\n",
    "            details = details.union(d['features'])\n",
    "\n",
    "    same_root = {}\n",
    "    for x in corrections:\n",
    "        roots = dictionary[x]['raiz']\n",
    "\n",
    "        for root in roots:\n",
    "            if root in same_root:\n",
    "                same_root[root].append(x)\n",
    "            else:\n",
    "                same_root[root] = [x]\n",
    "\n",
    "    characteristics = extract_details(details)\n",
    "    pruned_corrections = []\n",
    "    for _, similar_words in same_root.items():            \n",
    "        if len(similar_words) == 1:\n",
    "            pruned_corrections.append(similar_words[0])\n",
    "            continue\n",
    "        \n",
    "        score = {}\n",
    "        best_combination = 0\n",
    "        for word in similar_words:\n",
    "            score[word] = 0\n",
    "            word_details = extract_details(dictionary[word]['features'])\n",
    "            \n",
    "            for char, values in word_details.items():\n",
    "                if values[0] in characteristics[char]:\n",
    "                    score[word] += 1\n",
    "                    \n",
    "            if best_combination < score[word]:\n",
    "                best_combination = score[word]\n",
    "        \n",
    "        for word in similar_words:\n",
    "            if score[word] == best_combination:\n",
    "                pruned_corrections.append(word)\n",
    "\n",
    "    return pruned_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, corrections in possible_corrections.items():\n",
    "    print(prune_corrections(corrections, tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define uma classe para correção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corretor import Corretor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionary_path = \"./portilexicon-ud.tsv\"\n",
    "\n",
    "LXPARSER_WS_API_KEY = ''\n",
    "with open('.lxparser-key', 'r') as f: LXPARSER_WS_API_KEY = f.read()\n",
    "\n",
    "symbol_convertion = {\n",
    "    'A': ['ADJ'],\n",
    "    'ART': ['DET', 'PRON'],\n",
    "    'N': ['NOUN'],\n",
    "    'QNT': ['PRON'],\n",
    "    'ADV': ['ADV'],\n",
    "    'P': ['PRON'],\n",
    "    'REL': ['CCONJ', 'SCONJ'],\n",
    "    'V': ['VERB'],\n",
    "    'PRS': ['PRON'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_corretor = Corretor(dicionary_path)\n",
    "teste_corretor.setup_key(LXPARSER_WS_API_KEY)\n",
    "teste_corretor.setup_symbols(symbol_convertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_corretor.corrigir_texto(\"Cacharros quentes quentinhus aqui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação da solução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'paragraphs': array([array(['FotÃ³grafo retra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'paragraphs': array([array(['Giro UOL traz os...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'paragraphs': array([array(['29/03/2010 - 12h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'paragraphs': array([array(['Decisão do caso ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'paragraphs': array([array(['ÚLTIMAS NOTÍCIAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>{'paragraphs': array([array(['Holografias, \"hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>{'paragraphs': array([array(['Cresce nº de mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>{'paragraphs': array([array(['Com uma populaçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>{'paragraphs': array([array(['01/01/2010 - 23h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>{'paragraphs': array([array(['08/02/2010 - 18h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2840 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     {'paragraphs': array([array(['FotÃ³grafo retra...\n",
       "1     {'paragraphs': array([array(['Giro UOL traz os...\n",
       "2     {'paragraphs': array([array(['29/03/2010 - 12h...\n",
       "3     {'paragraphs': array([array(['Decisão do caso ...\n",
       "4     {'paragraphs': array([array(['ÚLTIMAS NOTÍCIAS...\n",
       "...                                                 ...\n",
       "2835  {'paragraphs': array([array(['Holografias, \"hu...\n",
       "2836  {'paragraphs': array([array(['Cresce nº de mul...\n",
       "2837  {'paragraphs': array([array(['Com uma populaçã...\n",
       "2838  {'paragraphs': array([array(['01/01/2010 - 23h...\n",
       "2839  {'paragraphs': array([array(['08/02/2010 - 18h...\n",
       "\n",
       "[2840 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./noticias.brwac.csv').drop(['id', 'title', 'uri'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array(l, *args, **kwargs):\n",
    "  return l\n",
    "\n",
    "def prepare_text(df, seed=42):\n",
    "  random.seed(seed)\n",
    "\n",
    "  sites_text = [eval(x) for x in df['text'].tolist()]\n",
    "  texts = [[p for p in paragraphs['paragraphs'] if isinstance(p, list)] for paragraphs in sites_text]\n",
    "  \n",
    "  texts = [x for paragraphs in texts for phrases in paragraphs for x in phrases if len(x) > 15]\n",
    "\n",
    "  texts = random.sample(texts, 1000)\n",
    "\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Na região cervical (pescoço), quer na parte da frente, quer na parte de trás, não consta assinalada nenhuma lesão.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = prepare_text(df)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(words):\n",
    "    for word in words:\n",
    "        if any(char.isdigit() for char in word) or len(word) < 4:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def corrupt_text(text, number=1, seed=42):\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Remove pontuação\n",
    "    words = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    original_words = random.choices(words, k=number)\n",
    "    \n",
    "    i = 0\n",
    "    while not check_validity(original_words):\n",
    "        if i > 100:\n",
    "            raise RuntimeError('Não há palavras válidas o suficiente')\n",
    "        original_words = random.choices(words, k=number)\n",
    "        i += 1\n",
    "\n",
    "    corruptions = []\n",
    "    for word in original_words:\n",
    "        corrupt_word = list(word)\n",
    "\n",
    "        corrupt_word[random.randint(\n",
    "            0, len(corrupt_word) - 1)] = random.choice(string.ascii_letters)\n",
    "        corruptions.append([word, ''.join(corrupt_word)])\n",
    "\n",
    "    return corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptions = {}\n",
    "for text in texts:\n",
    "    corruptions[text] = corrupt_text(texts[0], number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corretor import Corretor\n",
    "\n",
    "dicionary_path = \"./portilexicon-ud.tsv\"\n",
    "\n",
    "LXPARSER_WS_API_KEY = ''\n",
    "with open('.lxparser-key', 'r') as f: LXPARSER_WS_API_KEY = f.read()\n",
    "\n",
    "symbol_convertion = {\n",
    "    'A': ['ADJ'],\n",
    "    'ART': ['DET', 'PRON'],\n",
    "    'N': ['NOUN'],\n",
    "    'QNT': ['PRON'],\n",
    "    'ADV': ['ADV'],\n",
    "    'P': ['PRON'],\n",
    "    'REL': ['CCONJ', 'SCONJ'],\n",
    "    'V': ['VERB'],\n",
    "    'PRS': ['PRON'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Carregando dicionário: 100%|██████████| 1226339/1226339 [00:31<00:00, 39426.47it/s]\n"
     ]
    }
   ],
   "source": [
    "teste_corretor = Corretor(dicionary_path, key=LXPARSER_WS_API_KEY, symbols=symbol_convertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_texto = texts[0]\n",
    "for corruption in corruptions[teste_texto]:\n",
    "    teste_texto = teste_texto.replace(corruption[0], corruption[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Na região cervical (pescoço), Vuer na parte da frente, quer na parte de trás, não consta assinalada nenhuma lesão.\n",
      "Na região cervical pescoço, Vuer na parte da frente, quer na parte de trás, não consta assinalada nenhuma lesão.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(teste_texto)\n",
    "teste_texto = teste_texto.translate(str.maketrans('', '', string.punctuation.replace(',', '').replace('.', '')))\n",
    "\n",
    "print(teste_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        ROOT                                    \n",
      "                         |                                       \n",
      "                         S                                      \n",
      "                         |                                       \n",
      "                         VP                                     \n",
      "                         |                                       \n",
      "                         PP                                     \n",
      "  _______________________|_____                                  \n",
      " |                             NP                               \n",
      " |            _________________|_____                            \n",
      " |           NP                      NP                         \n",
      " |        ___|___     _______________|_____                      \n",
      " |       NP      |   |                     NP                   \n",
      " |    ___|___    |   |                     |                     \n",
      " |   NP      |   |   |                     N'                   \n",
      " |   |       |   |   |    _________________|_________________    \n",
      " P  PRS     PNT PNT PNT  N     N     N     N     N     N     N  \n",
      " |   |       |   |   |   |     |     |     |     |     |     |   \n",
      "de_ ela      !   ?   '   _   -lrb- -rrb- -lsb- -rsb- -lcb- -rcb-\n",
      "\n",
      "Lengths don't match: 1 != 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste_corretor.corrigir_texto(\"dela!?'_()[]{}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
