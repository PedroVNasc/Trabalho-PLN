{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solução simbólica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação do dicionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import TypedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(TypedDict):\n",
    "    raiz: set[str]\n",
    "    tag: set[str]\n",
    "    features: set[str]\n",
    "\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, head: str, data: dict | None = None, prox: dict[str, 'Node'] | None = None):\n",
    "        self.head = head\n",
    "        self.data = data\n",
    "        self.prox = {} if prox is None else prox\n",
    "\n",
    "\n",
    "class ReGra():\n",
    "    def __init__(self):\n",
    "        self.nodes: dict[str, Node] = {}\n",
    "\n",
    "    def dive(self, current_node: Node, word: str, depth: int):\n",
    "        # Palavra não existe ou palavra encontrada\n",
    "        if depth >= len(word) or word[depth] not in current_node.prox.keys():\n",
    "            return current_node\n",
    "\n",
    "        # Continua a recursão\n",
    "        return self.dive(current_node.prox[word[depth]], word, depth + 1)\n",
    "\n",
    "    def get_parent(self, word: str) -> Node | None:\n",
    "        if word[0] not in self.nodes.keys():\n",
    "            return None\n",
    "\n",
    "        return self.dive(self.nodes[word[0]], word, 1)\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        if word[0] not in self.nodes.keys():\n",
    "            return None\n",
    "\n",
    "        node = self.dive(self.nodes[word[0]], word, 1)\n",
    "\n",
    "        if node.head != word:\n",
    "            return None\n",
    "\n",
    "        return node.data\n",
    "\n",
    "    def __setitem__(self, idx: str, value: WordData):\n",
    "        # #print()\n",
    "        # print()\n",
    "        parent = self.get_parent(idx)\n",
    "\n",
    "        # Se o conjunto está vazio ou uma letra base não existe\n",
    "        if parent is None:\n",
    "            if len(idx) == 1:\n",
    "                self.nodes[idx] = Node(head=idx, data=value)\n",
    "                return\n",
    "\n",
    "            parent = Node(head=idx[0])\n",
    "            self.nodes[idx[0]] = parent\n",
    "\n",
    "            for i in range(1, len(idx) - 1):\n",
    "                parent.prox[idx[i]] = Node(head=idx[:i + 1])\n",
    "                parent = parent.prox[idx[i]]\n",
    "\n",
    "            parent.prox[idx[-1]] = Node(head=idx, data=value)\n",
    "            return\n",
    "\n",
    "        # Se a palavra existe, atualiza os dados\n",
    "        if parent.head == idx:\n",
    "            if parent.data is None:\n",
    "                parent.data = value\n",
    "\n",
    "            else:\n",
    "                parent.data['raiz'] = parent.data['raiz'].union(value['raiz'])\n",
    "                parent.data['tag'] = parent.data['tag'].union(\n",
    "                    value['tag'])\n",
    "                parent.data['features'] = parent.data['features'].union(\n",
    "                    value['features'])\n",
    "\n",
    "            return\n",
    "\n",
    "        # Se a palavra não existe, mas é filha de uma que existe\n",
    "        # cria o nó dela\n",
    "        if len(parent.head) + 1 == len(idx):\n",
    "            parent.prox[idx[-1]] = Node(head=idx, data=value)\n",
    "\n",
    "        # Se a palavra não existe, e não é filha de uma que existe\n",
    "        # cria um caminho de nós até ela\n",
    "        else:\n",
    "            for i in range(len(parent.head), len(idx) - 1):\n",
    "                parent.prox[idx[i]] = Node(head=idx[:i + 1])\n",
    "                parent = parent.prox[idx[i]]\n",
    "\n",
    "            parent.prox[idx[-1]] = Node(head=idx, data=value)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        string = ''\n",
    "        for node in self.nodes.values():\n",
    "            string += self.build_tree(node)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def build_tree(self, current: Node, depth: int = 0):\n",
    "        string = f\"{'-' * depth} {current.head}{' OK' if current.data is not None else ''}\\n\"\n",
    "\n",
    "        for node in current.prox.values():\n",
    "            string += self.build_tree(node, depth + 1)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def get_children(self, current: Node, max_depth: int = 10, depth: int = 0) -> list[str]:\n",
    "        if depth >= max_depth:\n",
    "            return []\n",
    "\n",
    "        children = []\n",
    "\n",
    "        for node in current.prox.values():\n",
    "            if node.data is not None:\n",
    "                children.append(node.head)\n",
    "\n",
    "            children += self.get_children(node, max_depth=max_depth, depth=depth + 1)\n",
    "\n",
    "        return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionary_path = \"./portilexicon-ud.tsv\"\n",
    "dictionary = ReGra()\n",
    "\n",
    "with open(dicionary_path) as dic:\n",
    "    rd = csv.reader(dic, delimiter='\\t', quotechar='\"')\n",
    "    qtd_lines = sum(1 for _ in rd)\n",
    "    dic.seek(0)    \n",
    "    print('teste')\n",
    "    \n",
    "    progress = tqdm(total=qtd_lines, desc='Carregando dicionário')\n",
    "    \n",
    "    for row in rd:\n",
    "        word = row[0]\n",
    "        raiz = [row[1]]\n",
    "        tag = [row[2]]\n",
    "        features = [x for x in row[3].split('|') if x]\n",
    "\n",
    "        dictionary[word] = WordData(raiz=set(raiz), tag=set(tag), features=set(features))\n",
    "        \n",
    "        progress.update(1)\n",
    "        \n",
    "    progress.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing da Frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    return text.lower().replace(',', '').replace(\n",
    "        '.', '').replace('!', '').replace('?', '').replace(';', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(phrase: str, dictionary: ReGra):\n",
    "    phrase = clean_text(phrase)\n",
    "    \n",
    "    out = {}\n",
    "    for word in phrase.split():\n",
    "        out[word] = dictionary[word]\n",
    "        \n",
    "        if out[word] is None:\n",
    "            out[word] = {'raiz': set(['Unknown']), 'tag': set(['Unknown']), 'features': set(['Unknown'])}\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração da Árvore Sintática\n",
    "\n",
    "A árvore sintática é gerada através do LX-Parser. Para obter um chave da API siga as instruções em:\n",
    "\n",
    "[https://portulanclarin.net/workbench/lx-parser/](https://portulanclarin.net/workbench/lx-parser/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LXPARSER_WS_API_URL = 'https://portulanclarin.net/workbench/lx-parser/api/'\n",
    "\n",
    "LXPARSER_WS_API_KEY = ''\n",
    "with open('.lxparser-key', 'r') as f: LXPARSER_WS_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função fornecida pelos desenvolvedores do LX-Parser\n",
    "\n",
    "class WSException(Exception):\n",
    "    'Webservice Exception'\n",
    "    def __init__(self, errordata):\n",
    "        \"errordata is a dict returned by the webservice with details about the error\"\n",
    "        super().__init__(self)\n",
    "        assert isinstance(errordata, dict)\n",
    "        self.message = errordata[\"message\"]\n",
    "        # see https://json-rpc.readthedocs.io/en/latest/exceptions.html for more info\n",
    "        # about JSON-RPC error codes\n",
    "        if -32099 <= errordata[\"code\"] <= -32000:  # Server Error\n",
    "            if errordata[\"data\"][\"type\"] == \"WebServiceException\":\n",
    "                self.message += f\": {errordata['data']['message']}\"\n",
    "            else:\n",
    "                self.message += f\": {errordata['data']!r}\"\n",
    "    def __str__(self):\n",
    "        return self.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função fornecida pelos desenvolvedores do LX-Parser\n",
    "\n",
    "def parse(text, format):\n",
    "    '''\n",
    "    Arguments\n",
    "        text: a string with a maximum of 2000 characters, Portuguese text, with\n",
    "             the input to be processed\n",
    "        format: either 'parentheses', 'table' or 'JSON'\n",
    "\n",
    "    Returns a string or JSON object with the output according to specification in\n",
    "       https://portulanclarin.net/workbench/lx-parser/\n",
    "    \n",
    "    Raises a WSException if an error occurs.\n",
    "    '''\n",
    "\n",
    "    request_data = {\n",
    "        'method': 'parse',\n",
    "        'jsonrpc': '2.0',\n",
    "        'id': 0,\n",
    "        'params': {\n",
    "            'text': text,\n",
    "            'format': format,\n",
    "            'key': LXPARSER_WS_API_KEY,\n",
    "        },\n",
    "    }\n",
    "    request = requests.post(LXPARSER_WS_API_URL, json=request_data)\n",
    "    response_data = request.json()\n",
    "    if \"error\" in response_data:\n",
    "        raise WSException(response_data[\"error\"])\n",
    "    else:\n",
    "        return response_data[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_teste = \"Cachorros quentes quentinhus da aqui\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = parse(frase_teste, format=\"parentheses\")\n",
    "print(result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import svgling\n",
    "from nltk import Tree\n",
    "\n",
    "tree: Tree = []\n",
    "for sentence in result_table.splitlines(keepends=False):\n",
    "    tree = Tree.fromstring(sentence)\n",
    "    IPython.display.display(svgling.draw_tree(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alinhando o dicionário e o LX-Parser\n",
    "\n",
    "Como eles utilizam símbolos diferentes, é necessário alinhar as classes antes de prosseguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_convertion = {\n",
    "    'A': ['ADJ'],\n",
    "    'ART': ['DET', 'PRON'],\n",
    "    'N': ['NOUN'],\n",
    "    'QNT': ['PRON'],\n",
    "    'ADV': ['ADV'],\n",
    "    'P': ['PRON'],\n",
    "    'REL': ['CCONJ', 'SCONJ'],\n",
    "    'V': ['VERB'],\n",
    "    'PRS': ['PRON'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [[x[0], symbol_convertion[x[1]]] for x in tree.pos() if x[1] in symbol_convertion]\n",
    "\n",
    "lx_parsed = []\n",
    "\n",
    "for x in result:\n",
    "    if x[0][-1] == '_':\n",
    "        x[0] = x[0][:-1]\n",
    "\n",
    "    if x[0].isalpha():\n",
    "        lx_parsed.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_parsed = [[x[0], x[1]['tag']] for x in parsing(' '.join([x[0].lower() for x in lx_parsed]), dictionary).items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligning(dict_parsed, lx_parsed):\n",
    "    if len(dict_parsed) != len(lx_parsed):\n",
    "        print(f\"Lengths don't match: {len(dict_parsed)} != {len(lx_parsed)}\")\n",
    "        return []\n",
    "\n",
    "    aligned_classes = []\n",
    "\n",
    "    for i in range(len(dict_parsed)):\n",
    "        if dict_parsed[i][0] != lx_parsed[i][0].lower():\n",
    "            print(f\"Words don't match: {dict_parsed[i][0]} != {lx_parsed[i][0]}\")\n",
    "            return []\n",
    "\n",
    "        word = lx_parsed[i][0]\n",
    "        classes = dict_parsed[i][1].intersection(set(lx_parsed[i][1]))\n",
    "\n",
    "        if len(classes) == 0:\n",
    "            if 'Unknown' in dict_parsed[i][1]:\n",
    "                classes = set(['ERROR', lx_parsed[i][1][0]])\n",
    "\n",
    "            else:\n",
    "                classes = dict_parsed[i][1]\n",
    "\n",
    "        aligned_classes.append([word, classes])\n",
    "\n",
    "    return aligned_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_tree = aligning(dict_parsed, lx_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in aligned_tree: print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sugere as mudanças"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correção gramatical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(word1, word2):\n",
    "    len1 = len(word1)\n",
    "    len2 = len(word2)\n",
    "\n",
    "    matrix_len = max(len1, len2) + 1\n",
    "    matrix = [[0 for _ in range(matrix_len)] for _ in range(matrix_len)]\n",
    "\n",
    "    for i in range(matrix_len):\n",
    "        matrix[i][0] = i\n",
    "        matrix[0][i] = i\n",
    "\n",
    "    for i in range(1, matrix_len):\n",
    "        for j in range(1, matrix_len):\n",
    "            matrix[i][j] = min(matrix[i - 1][j], matrix[i][j - 1], matrix[i - 1][j - 1])\n",
    "            \n",
    "            if i > len1:\n",
    "                matrix[i][j] += 1\n",
    "            elif j > len2:\n",
    "                matrix[i][j] += 1\n",
    "            elif word1[i - 1] != word2[j - 1]:\n",
    "                matrix[i][j] += 1\n",
    "\n",
    "    return matrix[matrix_len - 1][matrix_len - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, tag, dictionary):\n",
    "    word = clean_text(word)\n",
    "    p = dictionary.get_parent(word)\n",
    "    \n",
    "    print(tag)\n",
    "\n",
    "    # Procura paralavras com 2 até dois caracteres a mais ou a menos\n",
    "    if len(p.head) > 2:\n",
    "        p = dictionary.get_parent(p.head[:-2])\n",
    "\n",
    "    max_depth = max(len(word) - len(p.head) + 2, 4.0)\n",
    "    children = dictionary.get_children(p, max_depth=max_depth)\n",
    "\n",
    "    corrections = []\n",
    "    for c in children:\n",
    "        if tag == 'ANY' or tag in dictionary[c]['tag']:\n",
    "            corrections.append(c)\n",
    "\n",
    "    corrections.sort(key=lambda x: levenshtein_distance(word, x))\n",
    "    return corrections[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_corrections = {}\n",
    "\n",
    "for w in aligned_tree:\n",
    "    if 'ERROR' in w[1]:\n",
    "        tag = list(w[1] - {'ERROR'})[0]\n",
    "        possible_corrections[w[0]] = get_corrections(w[0], tag, dictionary)\n",
    "        \n",
    "        if len(possible_corrections[w[0]]) == 0:\n",
    "            possible_corrections[w[0]] = get_corrections(w[0], 'ANY', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(possible_corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorte das melhores soluções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_details(details):\n",
    "    details_dict = {}\n",
    "\n",
    "    for x in details:\n",
    "        if x == '_':\n",
    "            continue\n",
    "\n",
    "        char, value = x.split('=')\n",
    "        if char in details_dict:\n",
    "            details_dict[char].append(value)\n",
    "        else:\n",
    "            details_dict[char] = [value]\n",
    "\n",
    "    return details_dict\n",
    "\n",
    "def prune_corrections(corrections, tree: Tree):\n",
    "    details = set()\n",
    "    for x in tree.leaves():\n",
    "        d = dictionary[x]\n",
    "\n",
    "        if d is not None:\n",
    "            details = details.union(d['features'])\n",
    "\n",
    "    same_root = {}\n",
    "    for x in corrections:\n",
    "        roots = dictionary[x]['raiz']\n",
    "\n",
    "        for root in roots:\n",
    "            if root in same_root:\n",
    "                same_root[root].append(x)\n",
    "            else:\n",
    "                same_root[root] = [x]\n",
    "\n",
    "    characteristics = extract_details(details)\n",
    "    pruned_corrections = []\n",
    "    for _, similar_words in same_root.items():            \n",
    "        if len(similar_words) == 1:\n",
    "            pruned_corrections.append(similar_words[0])\n",
    "            continue\n",
    "        \n",
    "        score = {}\n",
    "        best_combination = 0\n",
    "        for word in similar_words:\n",
    "            score[word] = 0\n",
    "            word_details = extract_details(dictionary[word]['features'])\n",
    "            \n",
    "            for char, values in word_details.items():\n",
    "                if values[0] in characteristics[char]:\n",
    "                    score[word] += 1\n",
    "                    \n",
    "            if best_combination < score[word]:\n",
    "                best_combination = score[word]\n",
    "        \n",
    "        for word in similar_words:\n",
    "            if score[word] == best_combination:\n",
    "                pruned_corrections.append(word)\n",
    "\n",
    "    return pruned_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, corrections in possible_corrections.items():\n",
    "    print(prune_corrections(corrections, tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define uma classe para correção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corretor import Corretor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionary_path = \"./portilexicon-ud.tsv\"\n",
    "\n",
    "LXPARSER_WS_API_KEY = ''\n",
    "with open('.lxparser-key', 'r') as f: LXPARSER_WS_API_KEY = f.read()\n",
    "\n",
    "symbol_convertion = {\n",
    "    'A': ['ADJ'],\n",
    "    'ART': ['DET', 'PRON'],\n",
    "    'N': ['NOUN'],\n",
    "    'QNT': ['PRON'],\n",
    "    'ADV': ['ADV'],\n",
    "    'P': ['PRON'],\n",
    "    'REL': ['CCONJ', 'SCONJ'],\n",
    "    'V': ['VERB'],\n",
    "    'PRS': ['PRON'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_corretor = Corretor(dicionary_path)\n",
    "teste_corretor.setup_key(LXPARSER_WS_API_KEY)\n",
    "teste_corretor.setup_symbols(symbol_convertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_corretor.corrigir_texto(\"Cachorros quentes quentinhus da aqui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação da solução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./noticias.brwac.csv').drop(['id', 'title', 'uri'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array(l, *args, **kwargs):\n",
    "  return l\n",
    "\n",
    "def prepare_text(df, seed=42):\n",
    "  random.seed(seed)\n",
    "\n",
    "  sites_text = [eval(x) for x in df['text'].tolist()]\n",
    "  texts = [[p for p in paragraphs['paragraphs'] if isinstance(p, list)] for paragraphs in sites_text]\n",
    "  \n",
    "  texts = [x for paragraphs in texts for phrases in paragraphs for x in phrases if len(x) > 15]\n",
    "\n",
    "  texts = random.sample(texts, 150)\n",
    "\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = prepare_text(df)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(words):\n",
    "    for word in words:\n",
    "        if any(char.isdigit() for char in word) or len(word) < 4:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def corrupt_text(text, number=1, seed=42):\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Remove pontuação\n",
    "    words = text.translate(str.maketrans('', '', string.punctuation.replace('-', ''))).split()\n",
    "    original_words = random.choices(words, k=number)\n",
    "    \n",
    "    i = 0\n",
    "    while not check_validity(original_words):\n",
    "        if i > 100:\n",
    "            raise RuntimeError('Não há palavras válidas o suficiente')\n",
    "        original_words = random.choices(words, k=number)\n",
    "        i += 1\n",
    "\n",
    "    corruptions = []\n",
    "    for word in original_words:\n",
    "        corrupt_word = list(word)\n",
    "\n",
    "        corrupt_word[random.randint(\n",
    "            0, len(corrupt_word) - 1)] = random.choice(string.ascii_letters)\n",
    "        corruptions.append([word, ''.join(corrupt_word)])\n",
    "\n",
    "    return corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptions = {}\n",
    "for text in texts:\n",
    "    corruptions[text] = corrupt_text(text, number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corretor import Corretor\n",
    "\n",
    "dicionary_path = \"./portilexicon-ud.tsv\"\n",
    "\n",
    "LXPARSER_WS_API_KEY = ''\n",
    "with open('.lxparser-key', 'r') as f: LXPARSER_WS_API_KEY = f.read()\n",
    "\n",
    "symbol_convertion = {\n",
    "    'A': ['ADJ'],\n",
    "    'ART': ['DET', 'PRON'],\n",
    "    'N': ['NOUN'],\n",
    "    'QNT': ['PRON'],\n",
    "    'ADV': ['ADV'],\n",
    "    'P': ['PRON'],\n",
    "    'REL': ['CCONJ', 'SCONJ'],\n",
    "    'V': ['VERB'],\n",
    "    'PRS': ['PRON'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_corretor = Corretor(dicionary_path, key=LXPARSER_WS_API_KEY, symbols=symbol_convertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for text in texts:\n",
    "    for corruption in corruptions[text]:\n",
    "        text_corrupt = text.replace(corruption[0], corruption[1], 1)\n",
    "        text_corrupt = text_corrupt.translate(str.maketrans(\n",
    "            '', '', string.punctuation.replace('-', '').replace(',', '').replace('.', '')))\n",
    "        \n",
    "        result.append(teste_corretor.corrigir_texto(text_corrupt.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(result)\n",
    "points = 0\n",
    "for text, answer in zip(texts, result):\n",
    "    print(text)\n",
    "    print(answer)\n",
    "    print(corruptions[text])\n",
    "    \n",
    "    if corruptions[text][0][1] not in answer.keys() or answer[corruptions[text][0][1]] is None:\n",
    "        total -= 1\n",
    "    elif corruptions[text][0][0] in answer[corruptions[text][0][1]]:\n",
    "        points += 1\n",
    "\n",
    "    print()\n",
    "# print(corruptions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score:\", points / total)\n",
    "print(\"Total:\", total)\n",
    "print(\"Points:\", points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
